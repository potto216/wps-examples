{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `%matplotlib notebook` command is a Jupyter magic command that allows interactive plots to be displayed in a Jupyter notebook. It is used to render plots in the notebook itself. However, it seems that you are working in Visual Studio Code, which is an IDE and not a Jupyter notebook. Therefore, `%matplotlib notebook` may not work as expected. Instead, you can use `%matplotlib inline` to display plots inline in the output cell of the VS Code editor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io.wavfile import read, write\n",
    "from scipy.signal import correlate, resample\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import math\n",
    "\n",
    "\n",
    "# Load the two WAV files\n",
    "fs1, raw_audio1 = read('chirp_stereo.wav')\n",
    "#fs2, raw_audio2 = read('/mnt/hgfs/share/demo/nrf53audiodk_cis_c2_close_stream1.wav')\n",
    "fs2, raw_audio2 = read('/mnt/hgfs/share/demo/nrf53audiodk_cis_c3_medium_stream1.wav')\n",
    "\n",
    "#fs2, raw_audio2 = read('/mnt/hgfs/share/demo/nrf53audiodk_cis_c4_far_stream1.wav')\n",
    "#fs2, raw_audio2 = read('/mnt/hgfs/share/demo/nrf53audiodk_cis_c6_far_devices_far_stream1.wav')\n",
    "\n",
    "# Check if the audio is stereo\n",
    "if len(raw_audio1.shape) == 2 and raw_audio1.shape[1] == 2:\n",
    "    raw_audio1 = raw_audio1[:, 0]\n",
    "   # right_channel = raw_audio1[:, 1]\n",
    "else:\n",
    "    print(\"The audio is not stereo.\")\n",
    "\n",
    "# Check if the audio is stereo\n",
    "if len(raw_audio2.shape) == 2 and raw_audio2.shape[1] == 2:\n",
    "    raw_audio2 = raw_audio2[:, 0]\n",
    "   # right_channel = raw_audio2[:, 1]\n",
    "else:\n",
    "    print(\"The audio is not stereo.\")\n",
    "\n",
    "# Ensure the sample rates are the same\n",
    "if fs1 != fs2:\n",
    "    print(f\"The two WAV files have different sample rates fs1 = {fs1} and fs2 = {fs2}.\")\n",
    "\n",
    "if fs1 > fs2:\n",
    "    # Step 2: Resample the audio\n",
    "    num_samples = int(len(raw_audio2) * fs1 / fs2)\n",
    "    audio2 = resample(raw_audio2, num_samples)\n",
    "    audio1 = raw_audio1\n",
    "    fs2 = fs1\n",
    "elif fs1 < fs2:\n",
    "    # Step 2: Resample the audio\n",
    "    num_samples = int(len(raw_audio1) * fs2 / fs1)\n",
    "    audio1 = resample(raw_audio1, num_samples)\n",
    "    audio2 = raw_audio2\n",
    "    fs1 = fs2\n",
    "print(f\"The length of audio1 is {len(audio1)} and the length of audio2 is {len(audio2)}.\")\n",
    "\n",
    "# Step 3: Normalize the audio\n",
    "# Find the correlation between the two signals\n",
    "# This will give an array that indicates for each lag value how well the signals match\n",
    "corr = correlate(audio2, audio1, 'valid')\n",
    "\n",
    "# Find the maximum correlation index\n",
    "# This index indicates the best alignment (lag) of the two signals\n",
    "lag = np.argmax(corr)\n",
    "print(f\"The lag is {lag} and the adjusted lag is {lag-len(audio1)}\")\n",
    "\n",
    "# plot the correlation\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(corr)\n",
    "plt.title(\"Correlation between the two audio signals\")\n",
    "plt.xlabel(\"Lag\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# If audio1 starts before audio2, then lag will be positive\n",
    "if lag > 0:\n",
    "    #audio2 = audio2[(lag-int(1.125*len(audio1))):]\n",
    "    audio2 = audio2[(lag):]\n",
    "    audio2 = audio2[:len(audio1)]\n",
    "\n",
    "# If audio2 starts before audio1, then lag will be negative\n",
    "elif lag < 0:\n",
    "    audio2 = audio2[-lag:]\n",
    "    audio1 = audio1[:len(raw_audio2)]\n",
    "\n",
    "# Trim a fixed number of samples from start and end (e.g., 1000 samples)\n",
    "trim_samples = 1000\n",
    "audio1 = audio1[trim_samples:-trim_samples]\n",
    "audio2 = audio2[trim_samples:-trim_samples]\n",
    "\n",
    "# Now audio1 and audio2 are aligned and have the same length\n",
    "# Save them if needed\n",
    "# write('aligned_file1.wav', fs1, audio1)\n",
    "# write('aligned_file2.wav', fs2, audio2)\n",
    "\n",
    "# create a plot of the two signals\n",
    "# create a plot of the two signals\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "gs = GridSpec(2, 1, figure=fig)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[1, 0], sharex=ax1)\n",
    "ax1.plot(audio1)\n",
    "ax2.plot(audio2)\n",
    "ax1.set_ylabel('Amplitude')\n",
    "ax2.set_ylabel('Amplitude')\n",
    "ax2.set_xlabel('Sample number')\n",
    "ax1.set_title('Audio signals')\n",
    "ax1.grid(True)\n",
    "ax2.grid(True)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(audio2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the spectrogram of the two signals\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "gs = GridSpec(2, 1, figure=fig)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[1, 0], sharex=ax1)\n",
    "ax1.specgram(audio1, Fs=fs1)\n",
    "ax2.specgram(audio2, Fs=fs2)\n",
    "ax1.set_ylabel(f'Frequency [Hz] and fs = {fs1} Hz')\n",
    "ax2.set_ylabel(f'Frequency [Hz] and fs = {fs2} Hz')\n",
    "ax2.set_xlabel('Time [sec]')\n",
    "ax1.set_title('Spectrogram of audio signals')\n",
    "ax1.grid(True)\n",
    "ax2.grid(True)\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mse(signal1, signal2):\n",
    "    return np.mean((signal1 - signal2) ** 2)\n",
    "\n",
    "def mae(signal1, signal2):\n",
    "    return np.mean(np.abs(signal1 - signal2))\n",
    "\n",
    "\n",
    "print(f\"Mean Square Error: {mse(audio1, audio2)}\")\n",
    "print(f\"Mean Absolute Error: {mae(audio1, audio2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_list = [\n",
    "    '/mnt/hgfs/share/demo/nrf53audiodk_cis_c2_close.csv',\n",
    "    '/mnt/hgfs/share/demo/nrf53audiodk_cis_c3_medium.csv',\n",
    "    '/mnt/hgfs/share/demo/nrf53audiodk_cis_c4_far.csv',\n",
    "    '/mnt/hgfs/share/demo/nrf53audiodk_cis_c6_far_devices_far.csv'\n",
    "]\n",
    "\n",
    "dataframes = []  # list to store individual dataframes\n",
    "\n",
    "# loop through the files and read them in with pandas\n",
    "for file in file_list:\n",
    "    data = pd.read_csv(file)\n",
    "    # add a column to the dataframe with the filename\n",
    "    data['filename'] = file\n",
    "    dataframes.append(data)\n",
    "\n",
    "# Concatenate all the dataframes in the list into a master dataframe\n",
    "master_data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "print(master_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".wps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
